# -*- coding: utf-8 -*-
"""Web Scrapping Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K6DxWvf_pw2UkQJSwseFWV_wKTd4_Suw
"""

import requests
from bs4 import BeautifulSoup

url = 'https://www.speedtest.net/global-index'

response = requests.get(url)

if response.status_code == 200:
    print('Successfully fetched the webpage...')

    soup = BeautifulSoup(response.content, 'html.parser')

    mobile_element = soup.find(id='column-mobileMedian')
    broadband_element = soup.find(id='column-fixedMedian')

    countries_summary_mobile = mobile_element.find(class_='countriesSummary').find_all('a')
    cities_summary_mobile = mobile_element.find(class_='citiesSummary').find_all('a')

    countries_summary_broadband = broadband_element.find(class_='countriesSummary').find_all('a')
    cities_summary_broadband = broadband_element.find(class_='citiesSummary').find_all('a')
else:
    print('Failed to fetch the webpage. Status code:', response.status_code)

def fetch_data(summary, soup_id, type):
    info = []

    failed_entries = []

    for link in summary:
        res = requests.get(f"https://www.speedtest.net{link.get('href')}")

        if res.status_code == 200:
          print('Successfully fetched the webpage for ->', link.text.strip(), f"https://www.speedtest.net{link.get('href')}")

          soup = BeautifulSoup(res.content, 'html.parser')

          data = soup.find(id=soup_id)

          rank = data.find(class_='headings').find(class_='rank').find(class_='number').text.strip()
          rank_change = data.find(class_='headings').find(class_='rank').find(class_='rank-change').text.strip()

          download = data.find(class_='headings').find(class_='download').find(class_='number').text.strip()

          upload = data.find(class_='headings').find(class_='upload').find(class_='number').text.strip()

          latency = data.find(class_='headings').find(class_='latency').find(class_='number').text.strip()

          info.append({
              type: link.text.strip(),
              "position": rank, # rank named column not allowed
              "rank_change": rank_change,
              "download": download,
              "upload": upload,
              "latency": latency
          })
        else:
          failed_entries.append(link)

          print('Failed to fetch the webpage for ->', link.text.strip())

    return info, failed_entries

countries_mobile_data, failed_country_mobile = fetch_data(countries_summary_mobile, 'column-mobileMedian', 'country')
cities_mobile_data, failed_city_mobile = fetch_data(cities_summary_mobile, 'column-mobileCitiesMedian', 'city')

countries_broadband_data, failed_country_broadband = fetch_data(countries_summary_broadband, 'column-fixedMedian', 'country')
cities_broadband_data, failed_city_broadband = fetch_data(cities_summary_broadband, 'column-fixedCitiesMedian', 'city')

import pandas as pd

countries_mobile_df = pd.DataFrame(countries_mobile_data)

csv_filename = 'countries_mobile_data.csv'

countries_mobile_df.to_csv(csv_filename, index=False)

print(f"Data has been written to '{csv_filename}' successfully.")

countries_mobile_df.head()

cities_mobile_df = pd.DataFrame(cities_mobile_data)

csv_filename = 'cities_mobile_data.csv'

cities_mobile_df.to_csv(csv_filename, index=False)

print(f"Data has been written to '{csv_filename}' successfully.")

cities_mobile_df.head()

countries_broadband_df = pd.DataFrame(countries_broadband_data)

csv_filename = 'countries_broadband_data.csv'

countries_broadband_df.to_csv(csv_filename, index=False)

print(f"Data has been written to '{csv_filename}' successfully.")

countries_broadband_df.head()

cities_broadband_df = pd.DataFrame(cities_broadband_data)

csv_filename = 'cities_broadband_data.csv'

cities_broadband_df.to_csv(csv_filename, index=False)

print(f"Data has been written to '{csv_filename}' successfully.")

cities_broadband_df.head()

# SQL CREATE TABLE statements for each DataFrame
sql_statements = []
table_data = [('countries_mobile_data', countries_mobile_df), ('cities_mobile_data', cities_mobile_df), ('countries_broadband_data', countries_broadband_df), ('cities_broadband_data', cities_broadband_df)]

database_name = 'web_scrapping'
create_db_statement = f'CREATE DATABASE IF NOT EXISTS `{database_name}`;'
use_db = f'USE `{database_name}`;'
sql_statements.append(create_db_statement)
sql_statements.append(use_db)

for df_name, df in table_data:
    columns = ', '.join([f'{col} VARCHAR(255)' for col in df.columns])
    sql_statement = f'CREATE TABLE {df_name} ({columns});'
    sql_statements.append(sql_statement)

# SQL INSERT INTO statements for each DataFrame
for df_name, df in table_data:
    insert_sql = f'INSERT INTO {df_name} ({", ".join(df.columns)}) VALUES '
    values = ', '.join([str(tuple(row)) for row in df.values])
    sql_statements.append(insert_sql + values + ';')

# SQL statements to a .sql file
sql_filename = 'database_setup.sql'
with open(sql_filename, 'w') as file:
    for sql_statement in sql_statements:
        file.write(sql_statement + '\n')

print(f"SQL statements have been written to '{sql_filename}' successfully.")

